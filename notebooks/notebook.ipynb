{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79e81e51",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run preprocessing (set FORCE_PREPROCESS=True to rebuild)\\nFORCE_PREPROCESS = False\\n\\nneed_build = not (\\n    os.path.isdir(os.path.join(OUTPUT_DIR, 'train')) and\\n    os.path.isdir(os.path.join(OUTPUT_DIR, 'val')) and\\n    os.path.isdir(os.path.join(OUTPUT_DIR, 'test'))\\n)\\n\\nif FORCE_PREPROCESS or need_build:\\n    print('Running preprocessing...')\\n    preprocess_dataset(RAW_IMAGES_DIR, RAW_LABELS_DIR, OUTPUT_DIR)\\nelse:\\n    print('Using existing processed splits at:', OUTPUT_DIR)\\n\\ntrain_dir = os.path.join(OUTPUT_DIR, 'train')\\nval_dir = os.path.join(OUTPUT_DIR, 'val')\\ntest_dir = os.path.join(OUTPUT_DIR, 'test')\\n\\nprint('train_dir:', train_dir)\\nprint('val_dir  :', val_dir)\\nprint('test_dir :', test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48cd54aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset Preprocessing (embedded from scripts/preprocess_dataset.py)\\nimport shutil\\n\\nIMG_EXTS = ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']\\n\\ndef _find_image_path(base_dir: str, stem: str):\\n    for ext in IMG_EXTS:\\n        p = os.path.join(base_dir, f\"{stem}{ext}\")\\n        if os.path.exists(p):\\n            return p\\n    return None\\n\\ndef preprocess_dataset(images_dir: str, labels_dir: str, output_dir: str):\\n    \"\"\"\\n    Map images into class folders per YOLO label files, preserving splits.\\n      output_dir/\\n        train/<class_id>/image.jpg\\n        val/<class_id>/image.jpg\\n        test/<class_id>/image.jpg\\n    \"\"\"\\n    os.makedirs(output_dir, exist_ok=True)\\n    totals = {'train': 0, 'val': 0, 'test': 0}\\n    missing_images = 0\\n\\n    for subset in ['train', 'val', 'test']:\\n        subset_images_dir = os.path.join(images_dir, subset)\\n        subset_labels_dir = os.path.join(labels_dir, subset)\\n        if not os.path.isdir(subset_images_dir) or not os.path.isdir(subset_labels_dir):\\n            print(f\"Skipping {subset}: missing images or labels directory\")\\n            continue\\n        label_files = [f for f in os.listdir(subset_labels_dir) if f.endswith('.txt')]\\n        for label_file in label_files:\\n            stem = label_file[:-4]\\n            label_path = os.path.join(subset_labels_dir, label_file)\\n            class_ids = set()\\n            with open(label_path, 'r') as fh:\\n                for line in fh:\\n                    line = line.strip()\\n                    if not line:\\n                        continue\\n                    parts = line.split()\\n                    if not parts:\\n                        continue\\n                    class_ids.add(parts[0])\\n            img_path = _find_image_path(subset_images_dir, stem)\\n            if not img_path:\\n                missing_images += 1\\n                continue\\n            for cid in class_ids:\\n                class_dir = os.path.join(output_dir, subset, str(cid))\\n                os.makedirs(class_dir, exist_ok=True)\\n                shutil.copy(img_path, os.path.join(class_dir, os.path.basename(img_path)))\\n                totals[subset] += 1\\n    print('Copy summary (copies, not unique images):', totals)\\n    if missing_images:\\n        print(f\"Missing images for {missing_images} label files (different extensions?)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f23459c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab/Local Setup and Paths\\n# - Set BASE_DATA_DIR to where your data lives.\\n# - Expects the following layout:\\n#     BASE_DATA_DIR/\\n#       images/{train,val,test}/image*.jpg|jpeg|png\\n#       labels/{train,val,test}/image*.txt   # YOLO format\\n#   The preprocessing below will create:\\n#       image_label_mapping/{train,val,test}/{class_id}/image*.jpg\\n\\nimport os\\n\\nIN_COLAB = 'COLAB_GPU' in os.environ or os.getenv('COLAB_RELEASE_TAG') is not None\\n\\n# UPDATE THIS IF NEEDED (Colab default points to /content/data)\\nBASE_DATA_DIR = '/content/data' if IN_COLAB else '/workspaces/ppe-compliance-detection/data'\\n\\nRAW_IMAGES_DIR = os.path.join(BASE_DATA_DIR, 'images')\\nRAW_LABELS_DIR = os.path.join(BASE_DATA_DIR, 'labels')\\nOUTPUT_DIR = os.path.join(BASE_DATA_DIR, 'image_label_mapping')\\n\\nprint('IN_COLAB:', IN_COLAB)\\nprint('RAW_IMAGES_DIR:', RAW_IMAGES_DIR, 'exists:', os.path.isdir(RAW_IMAGES_DIR))\\nprint('RAW_LABELS_DIR:', RAW_LABELS_DIR, 'exists:', os.path.isdir(RAW_LABELS_DIR))\\nprint('OUTPUT_DIR:', OUTPUT_DIR, 'exists:', os.path.isdir(OUTPUT_DIR))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc8bbefc",
      "metadata": {
        "id": "fc8bbefc"
      },
      "source": [
        "# PPE Compliance Detection Project\n",
        "\n",
        "This notebook demonstrates the development of a deep learning model for detecting Personal Protective Equipment (PPE) compliance in construction environments. The goal is to classify images into multiple classes, including both compliant and non-compliant scenarios.\n",
        "\n",
        "## Project Overview\n",
        "- **Dataset**: Images of workers wearing or missing PPE (e.g., helmets, gloves, vests).\n",
        "- **Objective**: Build a model to detect PPE compliance and identify missing equipment.\n",
        "- **Approach**: Use transfer learning with a pre-trained MobileNetV2 model.\n",
        "\n",
        "## Steps\n",
        "1. Data Loading and Preprocessing\n",
        "2. Model Definition and Compilation\n",
        "3. Model Training with Callbacks\n",
        "4. Evaluation and Visualization\n",
        "5. Model Export for Deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84dc695b",
      "metadata": {
        "id": "84dc695b"
      },
      "source": [
        "## Data Preparation and Cleaning\n",
        "\n",
        "In this section, we will prepare the dataset for training. This includes:\n",
        "- Applying data augmentation techniques to improve model generalization.\n",
        "- Normalizing the image data to ensure consistent input to the model.\n",
        "- Use generators to load data from directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2df81745",
      "metadata": {
        "id": "2df81745"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-18 07:57:26.980905: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2025-12-18 07:57:44.699742: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-12-18 07:58:02.678574: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
          ]
        }
      ],
      "source": [
        "# Import Required Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9caab06a",
      "metadata": {
        "id": "9caab06a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 0 classes.\n",
            "Found 0 images belonging to 0 classes.\n",
            "Found 0 images belonging to 0 classes.\n"
          ]
        }
      ],
      "source": [
        "# Load and Preprocess Data (using processed split folders)\\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\\n\\n# If paths are not defined (e.g., running this cell alone), set defaults\\nif 'train_dir' not in globals():\\n    BASE_DATA_DIR = '/content/data' if ('COLAB_GPU' in os.environ or os.getenv('COLAB_RELEASE_TAG')) else '/workspaces/ppe-compliance-detection/data'\\n    OUTPUT_DIR = os.path.join(BASE_DATA_DIR, 'image_label_mapping')\\n    train_dir = os.path.join(OUTPUT_DIR, 'train')\\n    val_dir = os.path.join(OUTPUT_DIR, 'val')\\n    test_dir = os.path.join(OUTPUT_DIR, 'test')\\n\\n# Data augmentation + MobileNetV2 preprocessing for training\\ntrain_datagen = ImageDataGenerator(\\n    preprocessing_function=preprocess_input,\\n    rotation_range=20,\\n    width_shift_range=0.2,\\n    height_shift_range=0.2,\\n    shear_range=0.2,\\n    zoom_range=0.2,\\n    horizontal_flip=True,\\n    fill_mode='nearest'\\n)\\n\\n# Only preprocessing for val/test\\nval_test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\\n\\n# Generators\\ntrain_generator = train_datagen.flow_from_directory(\\n    train_dir, target_size=(224, 224), batch_size=32, class_mode='categorical'\\n)\\n\\nval_generator = val_test_datagen.flow_from_directory(\\n    val_dir, target_size=(224, 224), batch_size=32, class_mode='categorical'\\n)\\n\\ntest_generator = val_test_datagen.flow_from_directory(\\n    test_dir, target_size=(224, 224), batch_size=32, class_mode='categorical', shuffle=False\\n)\\n\\nprint('Classes:', train_generator.class_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8590c9ba",
      "metadata": {
        "id": "8590c9ba"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "\n",
        "In this section, we will:\n",
        "- Visualize the distribution of classes in the dataset.\n",
        "- Analyze the image dimensions and aspect ratios.\n",
        "- Identify any potential issues, such as class imbalance or missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33a265b",
      "metadata": {
        "id": "e33a265b"
      },
      "outputs": [],
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Class distribution using counts from generator\n",
        "labels = list(train_generator.class_indices.keys())\n",
        "counts = np.bincount(train_generator.classes)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(len(labels)), counts, color='tab:blue')\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(ticks=range(len(labels)), labels=labels, rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Inspect image dimensions from a single batch (fast and representative)\n",
        "batch_x, batch_y = next(train_generator)\n",
        "print('Sample batch shape:', batch_x.shape)  # (batch, height, width, channels)\n",
        "\n",
        "heights = batch_x.shape[1]\n",
        "widths = batch_x.shape[2]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(['height', 'width'], [heights, widths], color=['tab:green', 'tab:orange'])\n",
        "plt.title('Sample Image Dimensions')\n",
        "plt.ylabel('Pixels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01606892",
      "metadata": {
        "id": "01606892"
      },
      "source": [
        "## Model Selection and Parameter Tuning\n",
        "\n",
        "In this section, we will:\n",
        "- Define the model architecture using transfer learning with MobileNetV2.\n",
        "- Compile the model with appropriate loss functions and metrics.\n",
        "- Tune hyperparameters such as learning rate, batch size, and number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02cf8be0",
      "metadata": {
        "id": "02cf8be0"
      },
      "outputs": [],
      "source": [
        "# Model Selection and Parameter Tuning\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "\n",
        "# Load the pre-trained MobileNetV2 model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers for classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(len(train_generator.class_indices), activation='softmax')(x)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762ce824",
      "metadata": {
        "id": "762ce824"
      },
      "source": [
        "## Model Training with Callbacks\n",
        "In this section, we will:\n",
        "- Includes early stopping and model checkpointing.\n",
        "- Trains the model for up to 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66278cc8",
      "metadata": {
        "id": "66278cc8"
      },
      "outputs": [],
      "source": [
        "# Model Training with Callbacks\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath='best_model.h5', save_best_only=True)\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=20,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a20874db",
      "metadata": {
        "id": "a20874db"
      },
      "source": [
        "## Evaluation and Visualization:\n",
        "In this section, we will:\n",
        "- Plots training and validation accuracy/loss.\n",
        "- Evaluates the model on the test set and prints the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a9387c3",
      "metadata": {
        "id": "0a9387c3"
      },
      "outputs": [],
      "source": [
        "# Evaluation and Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aece0af2",
      "metadata": {
        "id": "aece0af2"
      },
      "source": [
        "## Model Export for Deployment:\n",
        "In this section, we will save the trained model as ppe_compliance_model.h5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c770eab4",
      "metadata": {
        "id": "c770eab4"
      },
      "outputs": [],
      "source": [
        "# Model Export for Deployment\n",
        "\n",
        "# Save the trained model\n",
        "model.save('ppe_compliance_model.h5')\n",
        "print(\"Model saved as 'ppe_compliance_model.h5'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ppe-compliance-detection",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
